{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Calculate Association Rules\nThis notebook will examine the `events` data to find items that tend to be purchased together, and creates a matrix that reflects the strength of the releationship. This matrix is stored in the `associations` collection. \n\nRun the following cell to retrieve the shared configuration values that point to your instance of Cosmos DB."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"%run \"./Includes/Shared-Configuration\""},{"cell_type":"markdown","metadata":{},"source":["Run the following cell to create the read and write configurations to use when interacting with Cosmos DB using the Spark Connector."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"readEventsConfig = {\n\"Endpoint\" : cosmos_db_endpoint,\n\"Masterkey\" : cosmos_db_master_key,\n\"Database\" : cosmos_db_database,\n\"Collection\" : \"events\",\n\"SamplingRatio\" : \"1.0\",\n\"schema_samplesize\" : \"1000\",\n\"query_pagesize\" : \"2147483647\",\n}\n\nwriteAssociationsConfig = {\n\"Endpoint\" : cosmos_db_endpoint,\n\"Masterkey\" : cosmos_db_master_key,\n\"Database\" : cosmos_db_database,\n\"Collection\" : \"associations\"\n}"},{"cell_type":"markdown","metadata":{},"source":["Whenever you write data back to Cosmos DB, you will need to provide a schema for DataFrame to apply when writing. Run the following cell to define this schema object."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"# Schema used by the associations collection\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\nassociationsSchema = StructType([\n  StructField(\"created\",StringType(),True),\n  StructField(\"source\",StringType(),True),\n  StructField(\"target\",StringType(),True),\n  StructField(\"support\",DoubleType(),True),\n  StructField(\"confidence\",DoubleType(),True),\n  StructField(\"_attachments\",StringType(),True),\n  StructField(\"_etag\",StringType(),True),\n  StructField(\"_rid\",StringType(),True),\n  StructField(\"_self\",StringType(),True),\n  StructField(\"_ts\",IntegerType(),True),\n])"},{"cell_type":"markdown","metadata":{},"source":["## Execute the association rules calculation logic\nThe goal of this algorithm is to compute two metrics that indicate the strength of a relationship between a source item and a target item based on event history, and then save that matrix to the `associations` collection in Cosmos DB.\n\nThe algorithm begins with grouping events with a `buy` action into a transaction, grouping by the `sessionId`. This provides the set of items bough together. \n\nFor example, a transaction with two items would look like:\n`'404973': ['5512872', '4172430']` where `404973` is the sessionId that is used as the transactionId, and the the array contains the id's of the items bought ('5512872' and '4172430').\n\nNext, the alogirthm uses these transactions to count the number of times a transaction includes a single, specific item. The output of this looks like:\n\n``{frozenset({'3521164'}): 24, frozenset({'4846340'}): 21, frozenset({'4034354'}): 27`\n\nThe above indicates the number of times item '3521164' was purchased by itself (24 times). Then this set is filtered to only include items having more than certain quantity of buys (this number is computed to be around 24 buys). \n\nThen the algorithm computes the possible pairwise combination for each distinct item in a transaction. For example:\n\n`items: ['1985949', '4624424', '4048272'], combinations[('1985949', '4624424'), ('1985949', '4048272'), ('4624424', '4048272')]`\n\nIn the above, a transaction included the items '1985949', '4624424', '4048272'. This would result in the combinations ('1985949', '4624424'), ('1985949', '4048272') and ('4624424', '4048272'). It filters out combinations where the invidual items did not have enough purchases on their own. Then then it tallies how many times those pairs of items were purchased together. For example:\n\n`frozenset({'4425200', '1289401'}): 2, frozenset({'4052882', '1289401'}): 1`\n\nIn the above intermediate result, items '4425200' and '1289401' were bought together 2 times, whereas items '4052882' and '1289401' were only bought together once.\n\nFinally, the algorithm loops over each item in the one item set that had the minimum number of buys, it treats this item as the source item and will use the the other item in the pair-wise two item set as the target item to compute the confidence (strength of the relationship) for. This results in rules that look like:\n\n`(datetime.datetime(2019, 9, 21, 16, 55, 3, 33313), '4630562', '1211837', 0.04, 0.0004065040650406504)`\n\nThe above indicates that source item '4630562' has a relationship with target item '1211837' with a support (the percentage of times the pair is bought together out of all transactions) of 0.04 and a confidence (the ratio of times the pair is bought together relative to the times the source item is bought alone) of 0.0004065040650406504.\n\nRun the following cell to define calculation logic."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"import os\nfrom collections import defaultdict\nfrom itertools import combinations\nfrom datetime import datetime\n\n\ndef build_association_rules(writeConfig):\n    data = retrieve_buy_events()\n    data = generate_transactions(data)\n\n    data = calculate_support_confidence(data, 0.01)\n    \n    save_rules(data, writeConfig)\n\n\ndef retrieve_buy_events():\n    print(\"retrieving buy events\")\n    data = spark.sql(\"SELECT * FROM events WHERE event ='buy'\")\n    return data\n\n\ndef generate_transactions(data):\n    print(\"generating transactions\")\n    transactions = dict()\n\n    for row in data.collect():\n        transaction_id = row[\"sessionId\"]\n        if transaction_id not in transactions:\n            transactions[transaction_id] = []\n        transactions[transaction_id].append(row[\"contentId\"])\n\n    # print(\"transactions: \", transactions)\n    return transactions\n\ndef calculate_support_confidence(transactions, min_sup=0.01):\n    print(\"calculating support confidence\")\n    N = len(transactions)\n    \n    one_itemsets = calculate_itemsets_one(transactions, min_sup)\n    # print(\"one_itemsets\",one_itemsets)\n    \n    two_itemsets = calculate_itemsets_two(transactions, one_itemsets)\n    # print(\"two_itemsets\", two_itemsets)\n    \n    rules = calculate_association_rules(one_itemsets, two_itemsets, N)\n    print(\"rules: \", rules[0:2])\n    return sorted(rules)\n\n\ndef calculate_itemsets_one(transactions, min_sup=0.01):\n\n    N = len(transactions)\n\n    temp = defaultdict(int)\n    one_itemsets = dict()\n\n    for key, items in transactions.items():\n        for item in items:\n            # using a frozenset enables the set to be used as a key in the dictionary\n            inx = frozenset({item})\n            temp[inx] += 1\n\n    # remove all items that do not have enough support (enough buys).\n    print(\"Removing items with fewer than {0} buys\".format(min_sup * N))\n    for key, itemset in temp.items():\n        if itemset > min_sup * N:\n            one_itemsets[key] = itemset\n\n    return one_itemsets\n\n\ndef calculate_itemsets_two(transactions, one_itemsets):\n    two_itemsets = defaultdict(int)\n\n    for key, items in transactions.items():\n        items = list(set(items))  # remove duplications\n\n        if (len(items) > 2):\n            # calculate all combination pairs of items possible from the list of items in the transaction\n            # print(\"items: {0}, combinations{1}\".format(items, list(combinations(items, 2))))\n            for perm in combinations(items, 2):\n                if has_support(perm, one_itemsets):\n                    two_itemsets[frozenset(perm)] += 1\n        elif len(items) == 2:\n            if has_support(items, one_itemsets):\n                two_itemsets[frozenset(items)] += 1\n    return two_itemsets\n\n\ndef calculate_association_rules(one_itemsets, two_itemsets, N):\n    timestamp = datetime.now()\n\n    rules = []\n    for source, source_freq in one_itemsets.items():\n        for key, group_freq in two_itemsets.items():\n            if source.issubset(key):\n                target = key.difference(source)                \n                support = float(group_freq) / N\n                confidence = float(group_freq) / source_freq\n                #print(\"group_freq:\",group_freq,\"N:\",N,\"source_freq:\",source_freq, \"support:\",support, \"confidence\", confidence)\n                rules.append((timestamp, next(iter(source)), next(iter(target)),\n                              confidence, support))\n    return rules\n\n\ndef has_support(perm, one_itemsets):\n    return frozenset({perm[0]}) in one_itemsets and \\\n           frozenset({perm[1]}) in one_itemsets\n\n\ndef save_rules(rules, config):\n    print(\"saving rules...\")\n    newRows = []\n    for rule in rules:\n        from pyspark.sql import Row\n        newRows.append( \n            #created:string, source:string, target:string, support:double, confidence:double, _attachments:string, _etag:string, _rid:string, _self:string, _ts:integer\n            Row(rule[0], rule[1],rule[2], rule[3], rule[4], None,None,None,None,None)\n        )\n    parallelizeRows = spark.sparkContext.parallelize(newRows)\n    new_documents = spark.createDataFrame(parallelizeRows, associationsSchema)\n    new_documents.createOrReplaceTempView(\"newdocs\")\n    new_documents.write.format(\"com.microsoft.azure.cosmosdb.spark\").mode(\"overwrite\").options(**config).save()\n    print(\"Associations saved\")\n\ndef truncate_collection(config, partitionKey):\n    # delete any existing ratings\n    from azure.cosmos import cosmos_client\n    database_link = 'dbs/' + config['Database']\n    collection_link = database_link + '/colls/' + config['Collection']\n    client = cosmos_client.CosmosClient(url_connection=config['Endpoint'], auth={'masterKey': config['Masterkey']})\n\n    documentlist = list(client.ReadItems(collection_link, {'maxItemCount':10}))\n\n    print('Found {0} documents'.format(documentlist.__len__()))\n\n    options = {}\n    options['enableCrossPartitionQuery'] = True\n    options['maxItemCount'] = 5\n\n    for doc in documentlist:\n        print('Deleting Document Id: {0}'.format(doc['id']))\n        docLink = collection_link + '/docs/' + doc['id']\n        options['partitionKey'] = doc[partitionKey]\n        client.DeleteItem(docLink, options)"},{"cell_type":"markdown","metadata":{},"source":["Run the following cell to calculate the associations and save them to Cosmos DB. The association rules calculated will be used later in the website to drive the online (realtime) calculation of item recommendations."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"# import the Cosmos DB Python SDK\ndbutils.library.installPyPI('azure-cosmos', version='3.1.1')\n\n#print(\"Deleting existing implicit ratings...\")\ntruncate_collection(writeAssociationsConfig, \"source\")\n\n# Connect via Spark connector to create Spark DataFrame\nevents_df = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\").options(**readEventsConfig).load()\nevents_df.createOrReplaceTempView(\"events\")\n\nprint(\"Calculating association rules...\")\nbuild_association_rules(writeAssociationsConfig)"},{"cell_type":"markdown","metadata":{},"source":["You are finished with this notebook and can return to the lab guide."]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}