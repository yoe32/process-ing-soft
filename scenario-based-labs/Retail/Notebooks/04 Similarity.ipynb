{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Calculate Item to Item Similarity\nThis notebook will calculate the similarity between items and then stores the result in the `similarity` collection of Cosmos DB.\n\nRun the following cell to retrieve the shared configuration values that point to your instance of Cosmos DB."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"%run \"./Includes/Shared-Configuration\""},{"cell_type":"markdown","metadata":{},"source":["Run the following cell to create the read and write configurations to use when interacting with Cosmos DB using the Spark Connector."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"readRatingsConfig = {\n\"Endpoint\" : cosmos_db_endpoint,\n\"Masterkey\" : cosmos_db_master_key,\n\"Database\" : cosmos_db_database,\n\"Collection\" : \"ratings\",\n\"SamplingRatio\" : \"1.0\",\n\"schema_samplesize\" : \"1000\",\n\"query_pagesize\" : \"2147483647\",\n}\n\nwriteSimilarityConfig = {\n\"Endpoint\" : cosmos_db_endpoint,\n\"Masterkey\" : cosmos_db_master_key,\n\"Database\" : cosmos_db_database,\n\"Collection\" : \"similarity\"\n}"},{"cell_type":"markdown","metadata":{},"source":["Whenever you write data back to Cosmos DB, you will need to provide a schema for DataFrame to apply when writing. Run the following cell to define this schema object."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"# Schema used by the similarity collection\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\nsimilaritySchema = StructType([\n  StructField(\"sourceItemId\",StringType(),True),\n  StructField(\"targetItemId\",StringType(),True),\n  StructField(\"similarity\",DoubleType(),True),\n  StructField(\"_attachments\",StringType(),True),\n  StructField(\"_etag\",StringType(),True),\n  StructField(\"_rid\",StringType(),True),\n  StructField(\"_self\",StringType(),True),\n  StructField(\"_ts\",IntegerType(),True),\n])"},{"cell_type":"markdown","metadata":{},"source":["## Define the item similarity calculation logic\n\nThe logic below uses the user to item ratings previously created to calculate a score indicating the similarity between a source item and a target item.\n\nThe process begins by loading the ratings matrix and for each user to item rating, calculating a new normalized rating (to adjust for the user's bias). \n\nAn overlap matrix is calculated that identifies, for any pair of items, how many users rated both items. First, the normalized ratings matrix is converted to a boolean matrix. That is, if an item for a user has a rating (regardless of the value of the rating), it has a value of 1, otherwise it is zero. Then dot product of the normalized ratings matrix against its transpose is calculated. This yields a simpler matrix where the value each cell now contains the count of the number users who rated both items. Cells that don't have any overlap, have a value of zero.\n\nSeparately, the cosine similarity of the normalized ratings matrix is computed. \nIt's easiest to understand the cosine similarity calculation as being done between an item `i` and another item `j`. \nThe cosine similarity is a ratio:\n- The numerator is computed as the sum of the product of the normalized rating of item `i` multiplied with the rating of `j`, for all users who have provided ratings.\n- The denominator is computed as the square root of the sum of the squares of the normalized rating of item `i` multiplied by the square root of the sum of thesquares of the normalized rating of item `j`.\n\nIn Python, the logic uses the `cosine_similarity` method from scikit-learn to compute the similarity between items by providing it our normalized user-to-items ratings matrix.\n\nThe result is then filtered to remove entries with a similarity score lower than configured, and having an overlap in the overlap matrix of less than a configured overlap in quantity of ratings for the pair of items.\n\nJust before saving, any resulting similarities with scores less than the configured minimum similarity are removed, so that weaker similarities are not recommended.\n\nRun the following cell to define the logic."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"import os\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse import coo_matrix\nfrom datetime import datetime\n\nclass ItemSimilarityMatrixBuilder(object):\n\n    def __init__(self, min_overlap=15, min_sim=0.2):\n        self.min_overlap = min_overlap\n        self.min_sim = min_sim\n\n\n    def build(self, ratings, config):\n\n        print(\"Calculating similarities ... using {} ratings\".format(len(ratings)))\n        start_time = datetime.now()\n\n        print(\"Normalizing ratings matrix\")\n        ratings['rating'] = ratings['rating'].astype(float)\n        ratings['avg'] = ratings.groupby('userId')['rating'].transform(lambda x: normalize(x))\n\n        ratings['avg'] = ratings['avg'].astype(float)\n        ratings['userId'] = ratings['userId'].astype('category')\n        ratings['itemId'] = ratings['itemId'].astype('category')\n\n        #print(\"build : ratings for user '400004' = {0}\".format( ratings[ratings['userId'] == '400004'] ))\n\n        coo = coo_matrix((ratings['avg'].astype(float),\n                          (ratings['itemId'].cat.codes.copy(),\n                           ratings['userId'].cat.codes.copy())))\n\n        print(\"Calculating overlaps between the items\")\n        overlap_matrix = coo.astype(bool).astype(int).dot(coo.transpose().astype(bool).astype(int))\n\n        number_of_overlaps = (overlap_matrix > self.min_overlap).count_nonzero()\n        print(\"Overlap matrix leaves {} out of {} with {}\".format(number_of_overlaps,\n                                                                         overlap_matrix.count_nonzero(),\n                                                                         self.min_overlap))\n\n        print(\"Rating matrix (size {}x{}) finished, in {} seconds\".format(coo.shape[0],\n                                                                                 coo.shape[1],\n                                                                                 datetime.now() - start_time))\n\n        sparsity_level = 1 - (ratings.shape[0] / (coo.shape[0] * coo.shape[1]))\n        print(\"Sparsity level is {}\".format(sparsity_level))\n\n        start_time = datetime.now()\n        cor = cosine_similarity(coo, dense_output=False)\n        cor = cor.multiply(cor > self.min_sim)\n        cor = cor.multiply(overlap_matrix > self.min_overlap)\n\n        items = dict(enumerate(ratings['itemId'].cat.categories))\n        print('Correlation is finished, done in {} seconds'.format(datetime.now() - start_time))\n\n        self.save_similarities(cor, items, config)\n\n        return cor, items\n\n    def save_similarities(self, sm, index, config):\n        created=datetime.utcnow()\n        newRows = []\n        no_saved = 0\n\n        coo = coo_matrix(sm)\n        csr = coo.tocsr()\n\n        #print(f'{coo.count_nonzero()} similarities to save')\n        xs, ys = coo.nonzero()\n        for x, y in zip(xs, ys):\n\n            if x == y:\n                continue\n\n            sim = float(csr[x, y])\n\n            if sim < self.min_sim:\n                continue\n\n            from pyspark.sql import Row\n            newRows.append( \n                #sourceItemId:string, targetItemId:string, similarity:double, _attachments:string, _etag:string, _rid:string, _self:string, _ts:integer\n                Row(index[x], index[y], float(sim), None,None,None,None,None)\n            )\n\n        parallelizeRows = spark.sparkContext.parallelize(newRows)\n        new_documents = spark.createDataFrame(parallelizeRows, similaritySchema)\n        new_documents.write.format(\"com.microsoft.azure.cosmosdb.spark\").mode(\"overwrite\").options(**config).save()\n        print(\"Similarities saved\")\n\n\ndef calculateItemSimilarity(writeSimilarityConfig):\n  \n    print(\"Truncating similarity collection\")\n    truncate_collection(writeSimilarityConfig, \"sourceItemId\")\n  \n    print(\"Calculation of item similarity\")\n\n    all_ratings = load_all_ratings()\n\n    ItemSimilarityMatrixBuilder(min_overlap=3, min_sim=0.0).build(all_ratings, writeSimilarityConfig)\n\ndef normalize(x):\n    x = x.astype(float)\n    x_sum = x.sum()\n    x_num = x.astype(bool).sum()\n    x_mean = x_sum / x_num\n\n    if x_num == 1 or x.std() == 0:\n        return 0.0\n    return (x - x_mean) / (x.max() - x.min())\n\n\ndef truncate_collection(config, partitionKey):\n    # delete any existing ratings\n    from azure.cosmos import cosmos_client\n    database_link = 'dbs/' + config['Database']\n    collection_link = database_link + '/colls/' + config['Collection']\n    client = cosmos_client.CosmosClient(url_connection=config['Endpoint'], auth={'masterKey': config['Masterkey']})\n\n    documentlist = list(client.ReadItems(collection_link, {'maxItemCount':10}))\n\n    print('Found {0} documents'.format(documentlist.__len__()))\n\n    options = {}\n    options['enableCrossPartitionQuery'] = True\n    options['maxItemCount'] = 5\n\n    for doc in documentlist:\n        print('Deleting Document Id: {0}'.format(doc['id']))\n        docLink = collection_link + '/docs/' + doc['id']\n        options['partitionKey'] = doc[partitionKey]\n        client.DeleteItem(docLink, options)\n        \n  \ndef load_all_ratings(min_ratings=3):\n    print(\"load_all_ratings : entered\")\n\n    ratings_df = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\").options(**readRatingsConfig).load()\n    ratings_df.createOrReplaceTempView(\"ratings\")\n    \n    ratings = ratings_df.toPandas()\n\n    # create a dataframe that that for each user, provides the number of movies rated\n    \n    #user_count = ratings[['user_id', 'movie_id']].groupby('user_id').count()\n    user_count = spark.sql(\"SELECT userId, count(itemId) numReviews FROM ratings GROUP BY userId\")\n    user_count.createOrReplaceTempView(\"user_count\")\n    \n    #print(\"load_all_ratings : user_count = {0}\".format(ratings[['userId', 'itemId']]))\n\n    # select just the user ids that have enough ratings \n    user_ids = [row['userId'] for row in spark.sql(\"SELECT userId FROM user_count where numReviews > {0}\".format(min_ratings)).collect()] \n    \n    #print(\"load_all_ratings : user_ids ={0}\".format(user_ids))\n\n    # selects just the ratings from those users having enough ratings\n    ratings = ratings[ratings['userId'].isin(user_ids)]\n    ratings['rating'] = ratings['rating'].astype(float)\n\n    #print(\"load_all_ratings : ratings ={0}\".format(ratings))\n\n    return ratings\n\n\n"},{"cell_type":"markdown","metadata":{},"source":["In addition to the Spark Connector for Cosmos DB, this notebook also uses the Azure Cosmos DB Python SDK. Run the following cell to install it."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"# import the Cosmos DB Python SDK\ndbutils.library.installPyPI('azure-cosmos', version='3.1.1')"},{"cell_type":"markdown","metadata":{},"source":["Execute the similarity calculation by running the following cell."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"calculateItemSimilarity(writeSimilarityConfig)"},{"cell_type":"markdown","metadata":{},"source":["You are finished with this notebook and can return to the lab guide."]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}